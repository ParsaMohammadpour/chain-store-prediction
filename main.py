# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1weD6njgAYwEPdQo91WXp7TBL0FyxbF4-

# Importing libraries we are going to use

در اینجا صرفا library هایی را که از آنها استفاده می کنیم را import می کنیم.
البته بعضی از این library ها صرفا برای مقاطعی و یا برای تست کردن اضافه شده بودند و الآن مورد استفاده نیستند.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
from sklearn import preprocessing, svm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.impute import SimpleImputer
from sklearn.model_selection import learning_curve
import shap
import time

"""# Loading dataframes """



"""در این قسمت به load کردن dataframe ها می پردازیم.

### Train Dataset
"""

df = pd.read_csv('sales_train.csv')
df

"""### Shop Dataset"""

shops = pd.read_csv('shops.csv')
shops

"""### Categories Dataset"""

categories = pd.read_csv('item_categories.csv')
categories

"""### Items Dataset"""

items = pd.read_csv('items.csv')
items

"""# Merging all dataframes into one

در این قسمت تمام dataframe هایی را که در قسمت قبل load کرده بودیم با هم merge می کنیم تا برای هر record داده هایی نظیر item_category_id و .. را داشته باشیم. (البته در ادامه تنها از همین item_category_id استفاده می کنیم.)
"""

df = pd.merge(df, shops, on='shop_id')
df = pd.merge(df, items, on='item_id')
df = pd.merge(df, categories, on='item_category_id')
df

"""# Get Rid Of Useless Columns Like Names

در این قسمت ستون هایی مانند name ها را از dataframe حذف می کنیم.
این کار برای سبک تر شدن dataframe می باشد تا عملیات ها سریع تر انجام شوند.
"""

df = df.loc[:, ~df.columns.isin(['item_category_name', 'item_name', 'shop_name'])]
df

"""# Correcting columns with negative value

در این dataframe، ما تعدادی داده با فروش منفی و قیمت منفی داریم، که قطعا نمی توانند منفی باشند این دو مقدار.
پس کل dataframe را پیمایش می کنیم و داده هایی که منفی هستند را حذف می کنیم و به جای آن ها، np.nan قرار می دهیم.
سپس با استفاده از SimpleImputer به جای داده هایی که مقدار آن ها np.nan می باشد، مقدار میانگیبن آن ستون را قرار می دهیم.
"""

# replacing negative values of item_price & item_cnt_day with np.nan
df = df.applymap(lambda x: x if isinstance(x, str) or x >= 0 else np.nan)
print(df.columns[df.isna().any()].tolist())

imp = SimpleImputer(missing_values=np.NaN, strategy="mean")
df.item_price = imp.fit_transform(df['item_price'].values.reshape(-1, 1))[:,0]
df.item_cnt_day = imp.fit_transform(df['item_cnt_day'].values.reshape(-1, 1))[:,0]

print('Columns with np.Nan values:\n', df.columns[df.isna().any()].tolist())
print('df:\n')
df

"""# Adding Month Column

در این قسمت از روی ستون date، ستون month را می سازیم که نشان دهنده عدد ماه در سال می باشد.
این کار برای این است که ممکن است یک item، در یک فصل یا ماه خاصی از سال فروش بیشتری داشته باشد، بنابر این این ستون را ایجاد می کنیم.(که در ادامه همین ستون را one-hot می کنیم.)
"""

df['month'] = df['date'].str[3:5].astype(int)
df

"""# Finding Max Date And Max date_block_num

در این قسمت تمام تاریخ های غیر تکراری را به صورت sort شده چاپ می کنیم تا بتوانیم تاریخ ماه بعدی ای که باید تعداد فروش هر کالا در آن را حساب کنیم، داشته باشیم. (بتوانیم 
بفهمیم ماه چندم سال است.)
همچنین همین کار را هم با ستون date_block_num تکرار می کنیم تا شماره ماه بعدی را نیز داشته باشیم. در حالت قبل می خواستیم فقط ببینیم ماه چندم سال است ولی در این قسمت می خواهیم ببینیم که چندمین ماهی است که قرار است دیتاهایش را پیش بینی کنیم.)
"""

dates_list = [time.strftime('%Y.%m.%d', i) for i in sorted([time.strptime(i, '%d.%m.%Y') for i in df['date'].unique()])]
print('dates:\n', dates_list)
date_block_num_list = sorted(df['date_block_num'].unique())
print('date_block_num:\n', date_block_num_list)

"""# Making Price Dataframe To Be Used For Test Dataset

در این قسمت میانگین قیمت هر کالا را در هر کدام از مغازه های ارائه دهنده آن کالا به صورت یک dataframe نگه می داریم تا بتوانیم قیمت داده های تست را از روی این مقادیر مقدار دهی کنیم.
"""

price_df = df.groupby(['item_id', 'shop_id'])['item_price'].mean().reset_index(name='item_price')
price_df

"""# Finding Of Each Shop Sell Summation Of Each Item

ما در dataframe  اصلی مان، تعداد فروش هر کالا در هر روز را دایم. اما ما باید تعداد فروش هر کالا در هر ماه را پیش بینی کنیم پس با استفاده از یک دستور groupby مجموع فروش ماهانه هر کالا در هر فروشگاه را حساب می کنیم.
تیکه کدی که در پایین به صورت کامنت وجود دارد صرفا برای تست کردن این دستور gropupby است.
"""

groupby_keep_columns = ['item_id', 'item_category_id', 'shop_id', 'date_block_num', 'month', 'item_price']
df = df.groupby(groupby_keep_columns)['item_cnt_day'].sum().reset_index(name='total-sell')
df

# This Part Was Only Added For Test
# a = df.groupby(['item_id', 'item_category_id', 'shop_id', 'date_block_num', 'month', 'item_price'])['item_cnt_day'].sum().reset_index(name='total-sell')
# b = df.loc[df['item_id'] == 22145]
# b = b.loc[b['shop_id'] == 25]
# b = b.loc[b['date_block_num'] == 14]
# print(len(b))
# print(b)
# print('%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')
# b = a.loc[a['item_id'] == 22145]
# b = b.loc[b['shop_id'] == 25]
# b = b.loc[b['date_block_num'] == 14]
# print(len(b))
# print(b)

"""# One-Hot-Encode for month, shop & item_category_id

می دانیم که id کالا ها نسبت به یکدیگر برتری ندارند و بهتر است که به صورت ترتیبی نباشند چون ممکن است نشان دهنده برتری یکی نسبت به دیگری باشد در حالی که اینطور نیست (سر کلاس بحث شد)
به همین دلیل برای آن ها (item_id, shop_id, month, item_category_id) زا روش one-hot استفاده می کنیم.
برای خود item_id از این روش استافده نکردیم به دلیل اینکه تعداد ستون هایمان خیلی زیاد می شد.
"""

df = pd.get_dummies(df, prefix=['month', 'shop', 'category'], columns = ['month', 'shop_id', 'item_category_id'])
df

"""# Splitting Dataset Into Train & Test Part

در اینجا با استافده از train_test_split می آیم و dataframe اصلی را به دو قسمت train و test به نسبت 95 به 5 تقسیم می کنیم.
"""

print(df.columns)
columns = ['item_id', 'item_price', 'date_block_num']
[columns.append('month_' + str(i)) for i in range(1, 13)]
[columns.append('shop_' + str(i)) for i in range(0, 60)]
[columns.append('category_' + str(i)) for i in range(0,84)]
x = df.loc[:, columns]
y = df.loc[:, 'total-sell']
train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.05)
print('train_x:\n', len(train_x))
print('train_y:\n', len(train_y))
print('test_x:\n', len(test_x))
print('test_y:\n', len(test_y))

"""# Linear regression

در این قسمت ماژول  linear regression را می سازیم و با استفاده از آن مدل را train می کنیم.
سپس با استفاده از داده های test که در قسمت قبل از کل داده های train جدا کرده بودیم، مدلمان را ارزیابی می کنیم.
سپس برای نشان دادن کارایی مدل، نموداری از نتیجه پیش بینی مدل بر روی 100 تا از داده های test را بر روی نمودار نشان می دهیم.
"""

regr = LinearRegression()
# reger = Lasso(alpha=0.001)

regr.fit(train_x, train_y)
print("Score: \n", regr.score(test_x, test_y))

y_predict = regr.predict(test_x)
y_predict = np.around(y_predict).astype(int)

# The coefficients
print("Coefficients: \n", regr.coef_)
# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(test_y, y_predict))
# The coefficient of determination: 1 is perfect prediction
print("Coefficient of determination: %.2f" % r2_score(test_y, y_predict))

plt.plot([i for i in range(100)], y_predict[:100], color='red')
plt.plot([i for i in range(100)], test_y[:100], color='blue')
plt.show()

"""# Learning Curve

در این قسمت با سرپ بسیار زیاد learning curve مدل ساخته شده مان را رسم میکنیم.
(در این بخش و بخش استفاده از کتابخانه shap، به دلیل محدودیت memory  سیستم، مجمور شدیم که یک سری از عملیات ها را تنها بر روی 1000 تا از داده های train و یا test انجام دهیم. استفاده از colab هم ممکن نبود به دلیل اینکه نمی توانستیم در آن جا فایلی upload کنیم.)
"""

"""We Also Reduced Dataset lenght Because It Took Very Long To Answer With The Complete Dataset
    & My System Didn't Have Enough Memory For"""
# Create CV training and test scores for various training set sizes
train_sizes, train_scores, test_scores = learning_curve(LinearRegression(), 
                                                        train_x.iloc[:1000],
                                                        train_y.iloc[:1000],
                                                        # Number of folds in cross-validation
                                                        cv=10,
                                                        # Evaluation metric
#                                                         scoring='accuracy',
                                                        # Use all computer cores
                                                        n_jobs=-1, 
                                                        # 50 different sizes of the training set
                                                        train_sizes=np.linspace(0.01, 1.0, 50))

# Create means and standard deviations of training set scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)

# Create means and standard deviations of test set scores
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Draw lines
plt.plot(train_sizes, train_mean, '--', color="#111111",  label="Training score")
plt.plot(train_sizes, test_mean, color="#111111", label="Cross-validation score")

# Draw bands
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color="#DDDDDD")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color="#DDDDDD")

# Create plot
plt.title("Learning Curve")
plt.xlabel("Training Set Size"), plt.ylabel("Accuracy Score"), plt.legend(loc="best")
plt.tight_layout()
plt.show()

"""# SHAP Library For Evaluating

در این قسمت با ساتفاده از کتابخانه shap میبینیم که مدلمان چگونه عمل می کند.
برای این کار هم 4 تا از نمودار های موجود در کتابخانه shap را برای مدلمان رسم میکنیم.
همانطور که در نمودار ها هم قابل مشاهده است یک سری از feature ها تاثییرات بیشتری بر روی مدل می گذارند و یک سری ها تاثییرات کمتری دارند.
البته نحوه نشان دادن این موضوع در نمودار های مختلف متفاوت است.

### Initializing Models And Variables For Shap
"""

shap.initjs()
explainer = shap.KernelExplainer(regr.predict, test_x.iloc[:1000])
shap_values = explainer.shap_values(test_x.iloc[0,:])

"""### force_plot"""

shap.force_plot(explainer.expected_value, shap_values, test_x.iloc[0,:])

"""### summary_plot"""

# shap.summary_plot(shap_values, test_x.iloc[:1000], feature_names=columns, plot_type="bar")
# shap.summary_plot(explainer.expected_value, shap_values[0])
# shap.summary_plot(shap_values, test_x.iloc[:1000])
shap.summary_plot(explainer.shap_values(test_x.iloc[:1000], nsamples=100), test_x.iloc[:1000])

"""### bar plots"""

bar_explainer = shap.Explainer(regr, test_x.iloc[:1000])
bar_shap_values = bar_explainer(test_x.iloc[:1000])

"""#### bar plot ( mean(| shap_value |) )"""

shap.plots.bar(bar_shap_values, max_display=500)

"""#### bar plot ( shap_value )"""

shap.plots.bar(bar_shap_values[0], max_display=500)

"""# Predicting Test

در این قسمت از مدلمان برای پیش بینی داده های موجود در فایل test استفاده می کنیم.
اما ابتدا باید تغییراتی را بر روی فیال test اعمال کنیم به طوری که بتوانیم آن را به صورت یک dataframe به مدل linear regression مان بدهیم.

### Reading Test Set

در این قسمت dataframe را زا روی فایل می خوانیم.
"""

test_df = pd.read_csv('test.csv')
test_df

"""### Adding Item Category

در این قسمت item_category_id را برای هر record اضافه می کنیم با استفاده از merge کردن با dataframe عه items، اما فقط از item_id و item_category_id استفاده می کهنیم.
چون در مدلمان هم فقط از همین دو تا استفاده کردذیم.
"""

test_df = pd.merge(test_df, items.loc[:, ['item_id', 'item_category_id']], on='item_id')
test_df

"""### Making Price For Each Item In Each Shop

همغانطور که در بالاتر توضیح داده شد، در اینجا از price_df که میانگین قیمت کالای هر فروشگاه است، استفاده می کنیم و به هر record میانگین قیمت آن کالای فروشگاه در ماه های گذشته را می دهیم.
"""

test_df = pd.merge(test_df, price_df, on=['item_id', 'shop_id'])
test_df

"""### Adding month, shop, category & date_block_num columns

همانطور که در بالاتر یک بار max عه date ها و date_block_num را بدست آورده بودیم، الآن از آن استفاده می کنیم و ستون Month و date_block_num را برای این دیتای test می سازیم.
همچنین ستون shop_id, category_id, month را هم همانطور که برای دیتای train مان one-hot کرده بودیم، اینجا هم one-hot می کنیم.
"""

"""The month that we need to predict is 2015.11 with date_block_num 34"""
for i in range(0, 13):
    test_df['month_' + str(i)] = 0 if i!= 11 else 1

for i in range(0, 60):
    test_df['shop_' + str(i)] = np.where(test_df['shop_id'] == (i+1), 1, 0)
    
for i in range(0, 84):
    test_df['category_' + str(i)] = np.where(test_df['item_category_id'] == (i+1), 1, 0)
    
test_df['date_block_num'] = 34
test_df

"""# Fitting The Whole Dataset & Predictiong The Test

در این قسمت هم می خواستیم که یک بار دیگر مدلمان را train کنیم این بار با تمام دیتای train اما به دلیل محدودیت های سیستم این کار قابل انجام دادن نبود. (not enough memory می گرفتیم و همچنین در colab هم نمی تواسنیم چیزی upload کنیم. البته کد مربوط به این قسمت به صورت کامنت قرار داده شده است.)
پس در این قسمت تنها از مدلی که قبلا train کرده بودیم با استفاده تنها بخشی از data، کار می کنیم و نتیجه را پیش بینی می کنیم و در فایل prediction قرار می دهیم.
"""

# reg = LinearRegression()

# reg.fit(x, y)
# print("Score:\n", reg.score(x, y))

# final_predict = reg.predict(test_df.iloc[:, columns])

"""If we want to get a better result we shoul uncomment higher lines and comment next line.
    But I don't have enough space for this"""
final_predict = regr.predict(test_df.loc[:, columns])

final_predict = np.around(final_predict).astype(int)
final_predict = np.abs(final_predict)

test_df['total-sell'] = final_predict

test_df.loc[:, ['ID', 'shop_id', 'item_id', 'total-sell']].to_csv('prediction.csv')